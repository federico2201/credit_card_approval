{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","metadata":{"executionCancelledAt":null,"executionTime":4145,"lastExecutedAt":1730746860187,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()","outputsMetadata":{"0":{"height":238,"type":"dataFrame"}},"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"string"},{"name":"1","type":"string"},{"name":"2","type":"number"},{"name":"3","type":"string"},{"name":"4","type":"string"},{"name":"5","type":"string"},{"name":"6","type":"string"},{"name":"7","type":"number"},{"name":"8","type":"string"},{"name":"9","type":"string"},{"name":"10","type":"integer"},{"name":"11","type":"string"},{"name":"12","type":"integer"},{"name":"13","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":["b","a","a","b","b"],"1":["30.83","58.67","24.50","27.83","20.17"],"2":[0,4.46,0.5,1.54,5.625],"3":["u","u","u","u","u"],"4":["g","g","g","g","g"],"5":["w","q","q","w","w"],"6":["v","h","h","v","v"],"7":[1.25,3.04,1.5,3.75,1.71],"8":["t","t","t","t","t"],"9":["t","t","f","t","f"],"10":[1,6,0,5,0],"11":["g","g","g","g","s"],"12":[0,560,824,3,0],"13":["+","+","+","+","+"],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b</td>\n      <td>30.83</td>\n      <td>0.000</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.25</td>\n      <td>t</td>\n      <td>t</td>\n      <td>1</td>\n      <td>g</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>58.67</td>\n      <td>4.460</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>3.04</td>\n      <td>t</td>\n      <td>t</td>\n      <td>6</td>\n      <td>g</td>\n      <td>560</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>24.50</td>\n      <td>0.500</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>1.50</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>g</td>\n      <td>824</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b</td>\n      <td>27.83</td>\n      <td>1.540</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>3.75</td>\n      <td>t</td>\n      <td>t</td>\n      <td>5</td>\n      <td>g</td>\n      <td>3</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b</td>\n      <td>20.17</td>\n      <td>5.625</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.71</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>s</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":2}]},{"source":"This is a binary classification problem. Therefore a Logistic Regression model is used.","metadata":{},"cell_type":"markdown","id":"712866c2-d96f-44e3-a154-0822d8d38cfe"},{"source":"Perform basic EDA - understand data types and spot missing values","metadata":{},"cell_type":"markdown","id":"b5d20880-7f85-46cc-b4f3-5581f8c3151a"},{"source":"cc_apps.info() # column 1 is of type object even though it's populated with floats","metadata":{"executionCancelledAt":null,"executionTime":59,"lastExecutedAt":1730746860247,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"cc_apps.info() # column 1 is of type object even though it's populated with floats","outputsMetadata":{"0":{"height":458,"type":"stream"}}},"cell_type":"code","id":"e20a3b4e-ed30-41aa-96e0-ca153db4434a","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\n"}],"execution_count":3},{"source":"# Dig deeper into column 1 to make sure conversion to float is possible\ncc_apps[1].value_counts() # there are twelve '?' values which will prevent conversion\n\n# A Logistic Regression model cannot employ these values\n# Convert them to nulls and then drop them, since it cannot process null values either\ncc_apps[1] = cc_apps[1].replace('?', np.nan)\ncc_apps = cc_apps.dropna(subset=[1])\n\n# Convert to float\ncc_apps[1] = cc_apps[1].astype(float)","metadata":{"executionCancelledAt":null,"executionTime":60,"lastExecutedAt":1730746860307,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dig deeper into column 1 to make sure conversion to float is possible\ncc_apps[1].value_counts() # there are twelve '?' values which will prevent conversion\n\n# A Logistic Regression model cannot employ these values\n# Convert them to nulls and then drop them, since it cannot process null values either\ncc_apps[1] = cc_apps[1].replace('?', np.nan)\ncc_apps = cc_apps.dropna(subset=[1])\n\n# Convert to float\ncc_apps[1] = cc_apps[1].astype(float)","outputsMetadata":{"0":{"height":543,"type":"dataFrame"}}},"cell_type":"code","id":"2d820ab7-2c49-4a0e-809c-4974a64ef8f9","outputs":[],"execution_count":4},{"source":"# Get item description\ncc_apps.describe() # column 12 is way out of scale ","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1730746860359,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get item description\ncc_apps.describe() # column 12 is way out of scale ","outputsMetadata":{"0":{"height":322,"type":"dataFrame"}}},"cell_type":"code","id":"8a51bb8d-fba0-426e-b33b-60fc9f62f766","outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"string"},{"name":"1","type":"number"},{"name":"2","type":"number"},{"name":"7","type":"number"},{"name":"10","type":"number"},{"name":"12","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"1":[678,31.5681710914,11.9578624983,13.75,22.6025,28.46,38.23,80.25],"2":[678,4.7776253687,4.9972397221,0,1,2.75,7.4375,28],"7":[678,2.2092256637,3.3507552707,0,0.165,1,2.57375,28.5],"10":[678,2.4351032448,4.8969656512,0,0,0,3,67],"12":[678,1021.2404129794,5251.9714531288,0,0,5,395.5,100000],"index":["count","mean","std","min","25%","50%","75%","max"]}},"total_rows":8,"truncation_type":null},"text/plain":"               1           2           7           10             12\ncount  678.000000  678.000000  678.000000  678.000000     678.000000\nmean    31.568171    4.777625    2.209226    2.435103    1021.240413\nstd     11.957862    4.997240    3.350755    4.896966    5251.971453\nmin     13.750000    0.000000    0.000000    0.000000       0.000000\n25%     22.602500    1.000000    0.165000    0.000000       0.000000\n50%     28.460000    2.750000    1.000000    0.000000       5.000000\n75%     38.230000    7.437500    2.573750    3.000000     395.500000\nmax     80.250000   28.000000   28.500000   67.000000  100000.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>7</th>\n      <th>10</th>\n      <th>12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>678.000000</td>\n      <td>678.000000</td>\n      <td>678.000000</td>\n      <td>678.000000</td>\n      <td>678.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>31.568171</td>\n      <td>4.777625</td>\n      <td>2.209226</td>\n      <td>2.435103</td>\n      <td>1021.240413</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>11.957862</td>\n      <td>4.997240</td>\n      <td>3.350755</td>\n      <td>4.896966</td>\n      <td>5251.971453</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>13.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>22.602500</td>\n      <td>1.000000</td>\n      <td>0.165000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>28.460000</td>\n      <td>2.750000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>38.230000</td>\n      <td>7.437500</td>\n      <td>2.573750</td>\n      <td>3.000000</td>\n      <td>395.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>80.250000</td>\n      <td>28.000000</td>\n      <td>28.500000</td>\n      <td>67.000000</td>\n      <td>100000.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":5}],"execution_count":5},{"source":"Feature engineering","metadata":{},"cell_type":"markdown","id":"8faf4320-3b1e-42b4-84d8-9a4449229eb1"},{"source":"# Inspect target column (last one - 13)\ncc_apps[13].value_counts() # suppose + is approved, - is non-approved\n\n# Convert these values to binary\ncc_apps[13] = cc_apps[13].replace({'+':1, '-':0})\n\n# Convert field to integer\ncc_apps[13] = cc_apps[13].astype(int)","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1730746860412,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Inspect target column (last one - 13)\ncc_apps[13].value_counts() # suppose + is approved, - is non-approved\n\n# Convert these values to binary\ncc_apps[13] = cc_apps[13].replace({'+':1, '-':0})\n\n# Convert field to integer\ncc_apps[13] = cc_apps[13].astype(int)","outputsMetadata":{"0":{"height":137,"type":"dataFrame"}}},"cell_type":"code","id":"2e2380df-da0e-465a-98ce-00cc248a278a","outputs":[],"execution_count":6},{"source":"# Before training the model two transformations are pending\n    # Get dummies for object fields\n    # Scale numeric fields\n    \n# Get dummies\ncc_apps = pd.get_dummies(cc_apps, columns=[0, 3, 4, 5, 6, 8, 9, 11])\n\n# Scale numeric fields\n# First drop target field\nscaler = StandardScaler()\nfeatures = cc_apps.drop([13], axis=1)\n\n# Turn all column names to strings to apply Standard Scaler\nfeatures.columns = features.columns.astype(str)\n\n# Scale by creating a new dataframe, so as to avoid issues with column names in 'features'\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1730746860467,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Before training the model two transformations are pending\n    # Get dummies for object fields\n    # Scale numeric fields\n    \n# Get dummies\ncc_apps = pd.get_dummies(cc_apps, columns=[0, 3, 4, 5, 6, 8, 9, 11])\n\n# Scale numeric fields\n# First drop target field\nscaler = StandardScaler()\nfeatures = cc_apps.drop([13], axis=1)\n\n# Turn all column names to strings to apply Standard Scaler\nfeatures.columns = features.columns.astype(str)\n\n# Scale by creating a new dataframe, so as to avoid issues with column names in 'features'\nfeatures_scaled = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n"},"cell_type":"code","id":"443f1510-5989-47f9-a7fa-96d7ddfa859b","outputs":[],"execution_count":7},{"source":"Now train some models","metadata":{},"cell_type":"markdown","id":"72bee19f-555f-4046-94cf-b9335ecd43d0"},{"source":"# Define target data\ntarget = cc_apps[13]\n\n# Split data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.8, random_state=42)\n\n# Target data is binary - use Logistic Regression\n# Initialize\nlogreg = LogisticRegression(random_state=42)\n\n# Train and predict\nlogreg.fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\n\n# Evaluate\nprint(classification_report(y_test, logreg_pred)) # Accuracy ~.84 - more than satisfactory\nprint(confusion_matrix(y_test, logreg_pred))","metadata":{"executionCancelledAt":null,"executionTime":106,"lastExecutedAt":1730746860573,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define target data\ntarget = cc_apps[13]\n\n# Split data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.8, random_state=42)\n\n# Target data is binary - use Logistic Regression\n# Initialize\nlogreg = LogisticRegression(random_state=42)\n\n# Train and predict\nlogreg.fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\n\n# Evaluate\nprint(classification_report(y_test, logreg_pred)) # Accuracy ~.84 - more than satisfactory\nprint(confusion_matrix(y_test, logreg_pred))","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"01b44b91-8fed-4e40-afcb-ebb6bc0c4d4b","outputs":[{"output_type":"stream","name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.86      0.85      0.86       305\n           1       0.81      0.82      0.82       238\n\n    accuracy                           0.84       543\n   macro avg       0.84      0.84      0.84       543\nweighted avg       0.84      0.84      0.84       543\n\n[[260  45]\n [ 42 196]]\n"}],"execution_count":8},{"source":"Can accuracy still be improved?\nLet's try a Random Forest Classifier","metadata":{},"cell_type":"markdown","id":"f1d8c3bc-c5ef-4350-8afa-56e3843e30fc"},{"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize\nrf = RandomForestClassifier(random_state=42)\n\n# Train and predict\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\n# Evaluate\nprint(classification_report(y_test, rf_pred)) # Accuracy ~.87 - even better\nprint(confusion_matrix(y_test, rf_pred))","metadata":{"executionCancelledAt":null,"executionTime":280,"lastExecutedAt":1730746860853,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Initialize\nrf = RandomForestClassifier(random_state=42)\n\n# Train and predict\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\n# Evaluate\nprint(classification_report(y_test, rf_pred)) # Accuracy ~.87 - even better\nprint(confusion_matrix(y_test, rf_pred))","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"a04bccaa-0d24-4d34-8536-0d0f602b9ff7","outputs":[{"output_type":"stream","name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.85      0.88       305\n           1       0.82      0.88      0.85       238\n\n    accuracy                           0.87       543\n   macro avg       0.86      0.87      0.86       543\nweighted avg       0.87      0.87      0.87       543\n\n[[260  45]\n [ 28 210]]\n"}],"execution_count":9},{"source":"Random Forest Classifier proves to be a better model than Logistic Regression. However, we've only used vanilla versions of both models - we can still tweak their parameters to see if better accuracy can be achieved.\n\nLet's use GridSearch for it.","metadata":{},"cell_type":"markdown","id":"000d8b6e-a1d7-4c11-b775-51a5e65b6832"},{"source":"# Define parameters to test for both models\nlogreg_params = {\n    'C': [0.1, 1, 10],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga']\n}\n\nrf_params = {\n    'n_estimators': [100, 200, 500],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Try Logistic Regression first\n# Instantiate GridSearch object\ngrid_search_logreg = GridSearchCV(\n    estimator = logreg,\n    param_grid = logreg_params,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1    \n)\n\n# Fit grid\ngrid_search_logreg.fit(X_train, y_train)\n\n# Get best features\nlogreg_best_params = grid_search_logreg.best_params_\nlogreg_best_score = grid_search_logreg.best_score_\nlogreg_best_model = grid_search_logreg.best_estimator_\n\n# Now try Random Forest\n# Instantiate\ngrid_search_rf = GridSearchCV(\n    estimator = rf,\n    param_grid = rf_params,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1    \n)\n\n# Fit\ngrid_search_rf.fit(X_train, y_train)\n\n# Get best features\nrf_best_params = grid_search_rf.best_params_\nrf_best_score = grid_search_rf.best_score_\nrf_best_model = grid_search_rf.best_estimator_\n\n# Assess prediction capacity of both best models\nlogreg_pred_best = logreg_best_model.predict(X_test)\nlogreg_best_score = accuracy_score(y_test, logreg_pred_best)\nrf_pred_best = rf_best_model.predict(X_test)\nrf_best_score = accuracy_score(y_test, rf_pred_best)\n\n# Get values\nprint(logreg_best_score)\nprint(rf_best_score)\n","metadata":{"executionCancelledAt":null,"executionTime":87974,"lastExecutedAt":1730746948827,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define parameters to test for both models\nlogreg_params = {\n    'C': [0.1, 1, 10],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga']\n}\n\nrf_params = {\n    'n_estimators': [100, 200, 500],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Try Logistic Regression first\n# Instantiate GridSearch object\ngrid_search_logreg = GridSearchCV(\n    estimator = logreg,\n    param_grid = logreg_params,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1    \n)\n\n# Fit grid\ngrid_search_logreg.fit(X_train, y_train)\n\n# Get best features\nlogreg_best_params = grid_search_logreg.best_params_\nlogreg_best_score = grid_search_logreg.best_score_\nlogreg_best_model = grid_search_logreg.best_estimator_\n\n# Now try Random Forest\n# Instantiate\ngrid_search_rf = GridSearchCV(\n    estimator = rf,\n    param_grid = rf_params,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1    \n)\n\n# Fit\ngrid_search_rf.fit(X_train, y_train)\n\n# Get best features\nrf_best_params = grid_search_rf.best_params_\nrf_best_score = grid_search_rf.best_score_\nrf_best_model = grid_search_rf.best_estimator_\n\n# Assess prediction capacity of both best models\nlogreg_pred_best = logreg_best_model.predict(X_test)\nlogreg_best_score = accuracy_score(y_test, logreg_pred_best)\nrf_pred_best = rf_best_model.predict(X_test)\nrf_best_score = accuracy_score(y_test, rf_pred_best)\n\n# Get values\nprint(logreg_best_score)\nprint(rf_best_score)\n","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"2acb2325-0bb2-4c75-8c02-36f8f1b0406c","outputs":[{"output_type":"stream","name":"stdout","text":"0.856353591160221\n0.8637200736648251\n"}],"execution_count":10},{"source":"With GridSearch, overall accuracy has actually decreased by a bit, which can be early signs of overfitting in the model. With that in mind and having already achieved an accuracy larger than .75, the exercise stops here.\n\nAfter thoughts: some other binary classification models, like SVC and XGBoost, can also be considered. Further increase in accuracy may come from more complex feature engineering. Keep also in mind that other metrics, like f1 score, have not been studied.","metadata":{},"cell_type":"markdown","id":"34b83c0e-f4f1-433e-aa18-92d819cbeee2"},{"source":"best_score = accuracy_score(y_test, rf_pred)\nprint(best_score)","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1730746948883,"lastExecutedByKernel":"f589773d-2a92-4db4-8ef6-5a67f82514b6","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"best_score = accuracy_score(y_test, rf_pred)\nprint(best_score)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"438e7b04-140a-4380-b9ba-73891c4375be","outputs":[{"output_type":"stream","name":"stdout","text":"0.8655616942909761\n"}],"execution_count":11}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}